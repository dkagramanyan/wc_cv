{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2690cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import io, filters\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from mpire import WorkerPool\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from combra import stats as cstats\n",
    "from combra import approx as capprox\n",
    "from combra import image as cimage\n",
    "\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from combra.tests import test_fractal_dimensions\n",
    "from combra.contours import contour_to_binary_mask, scale_contour, draw_contours\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cafe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = img[400:-400,1300:-1300]\n",
    "    _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    cnts, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    return binary, cnts\n",
    "\n",
    "\n",
    "# --- Group images by number ranges (1-500, 501-1000, ...) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def _extract_image_number(p):\n",
    "    stem = Path(p).stem\n",
    "    m = re.search(r\"(\\d+)\", stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "images_dir = Path(\"data/autumn/images\")\n",
    "all_image_paths = [p for p in images_dir.glob(\"*.JPG\") if re.fullmatch(r\"\\d+\", p.stem)]\n",
    "all_image_paths = sorted(\n",
    "    all_image_paths,\n",
    "    key=lambda p: _extract_image_number(p) or 0,\n",
    ")\n",
    "\n",
    "# If you also have .jpg/.jpeg, uncomment:\n",
    "# all_image_paths += list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.jpeg\"))\n",
    "\n",
    "group_size = 500\n",
    "_groups = {}\n",
    "\n",
    "for p in all_image_paths:\n",
    "    n = _extract_image_number(p)\n",
    "    if n is None:\n",
    "        continue\n",
    "    g_start = ((n - 1) // group_size) * group_size + 1\n",
    "    g_end = g_start + group_size - 1\n",
    "    label = f\"{g_start:04d}-{g_end:04d}\"\n",
    "    _groups.setdefault(label, []).append(str(p))\n",
    "\n",
    "# Sorted list of (label, [image_path, ...])\n",
    "image_groups = sorted(\n",
    "    _groups.items(),\n",
    "    key=lambda kv: int(kv[0].split(\"-\")[0]),\n",
    ")\n",
    "\n",
    "group_labels = [label for label, _ in image_groups]\n",
    "\n",
    "print(f\"Found {len(all_image_paths)} images, grouped into {len(image_groups)} groups\")\n",
    "if image_groups:\n",
    "    print(\"First 5 groups:\", [(k, len(v)) for k, v in image_groups[:5]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5bde8",
   "metadata": {},
   "source": [
    "# Fractal dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = 2 ** np.arange(1, 10)\n",
    "test_fractal_dimensions(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b43b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "types=['fractal_dimension']\n",
    "type=types[0]\n",
    "\n",
    "n_jobs=20\n",
    "\n",
    "\n",
    "\n",
    "a_list = []\n",
    "\n",
    "n_images_list = []\n",
    "n_contours_list = []\n",
    "fd_mean_list = []\n",
    "fd_median_list = []\n",
    "fd_lists = []\n",
    "\n",
    "def _get_contours(image_path):\n",
    "    # return only contours (avoid transferring the binary image)\n",
    "    _, contours = preprocess_image(image_path)\n",
    "    return contours\n",
    "\n",
    "for idx, (group_label, group_paths) in tqdm(enumerate(image_groups)):\n",
    "\n",
    "    # Collect ALL contours from ALL images in this group (in parallel)\n",
    "    with WorkerPool(n_jobs=n_jobs, use_dill=False) as pool:\n",
    "        contours_per_image = pool.map(\n",
    "            _get_contours,\n",
    "            group_paths,\n",
    "            progress_bar=True,\n",
    "            chunk_size=1,\n",
    "        )\n",
    "\n",
    "        all_contours = [c for contours in contours_per_image for c in contours]\n",
    "\n",
    "        # Process in parallel - NO shared_objects (as in your 25-39 snippet)\n",
    "        results = pool.map(\n",
    "            cimage.contour_fractal_dimension,\n",
    "            all_contours,\n",
    "            progress_bar=True,\n",
    "            chunk_size=64,\n",
    "        )\n",
    "\n",
    "    fd_list = [d for d in results if d is not None]\n",
    "\n",
    "    # Keep some diagnostics\n",
    "    n_images_list.append(len(group_paths))\n",
    "    n_contours_list.append(len(all_contours))\n",
    "    fd_mean_list.append(float(np.mean(fd_list)) if fd_list else np.nan)\n",
    "    fd_median_list.append(float(np.median(fd_list)) if fd_list else np.nan)\n",
    "    fd_lists.append([float(x) for x in fd_list])\n",
    "\n",
    "# Save results to JSON (per-group)\n",
    "out_path = Path(\"fractals_results_full.json\")\n",
    "results_json = {\n",
    "    \"type\": type,\n",
    "    \"n_jobs\": n_jobs,\n",
    "    \"groups\": [\n",
    "        {\n",
    "            \"group_label\": gl,\n",
    "            \"n_images\": int(ni),\n",
    "            \"n_contours\": int(nc),\n",
    "            \"fd_mean\": None if (isinstance(fm, (float, np.floating)) and np.isnan(fm)) else float(fm),\n",
    "            \"fd_median\": None if (isinstance(fmd, (float, np.floating)) and np.isnan(fmd)) else float(fmd),\n",
    "            \"fd_list\": fdl,\n",
    "        }\n",
    "        for gl, ni, nc, fm, fmd, fdl in zip(\n",
    "            group_labels, n_images_list, n_contours_list, fd_mean_list, fd_median_list, fd_lists\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f070c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use JSON generated by the fractal-dimension extraction cell\n",
    "in_path = Path(\"fractals_results_full.json\")\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    results_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "groups = results_json[\"groups\"]\n",
    "group_labels = [g[\"group_label\"] for g in groups]\n",
    "\n",
    "# bin size for stats_preprocess\n",
    "for step in tqdm([0.01, 0.05, 0.1, 0.2, 0.3]):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Colors per group (will cycle if many groups)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, max(2, min(20, len(groups)))))\n",
    "\n",
    "    a_list = []\n",
    "\n",
    "    for idx, g in enumerate(groups):\n",
    "        fd_list = g.get(\"fd_list\", []) or []\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        if not fd_list:\n",
    "            a_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        x_orig, y_orig = cstats.stats_preprocess(fd_list, step)\n",
    "\n",
    "        # Exponential distribution only\n",
    "        (x_fit, y_fit), a, amp_exp = capprox.exponential_approx(\n",
    "            x_orig, y_orig, a=1, amp=1, x_lim=[1, 2], N=20\n",
    "        )\n",
    "        a_list.append(a)\n",
    "\n",
    "        label = (\n",
    "            # f\"{g['group_label']} | imgs={g.get('n_images', 0)} | cnts={g.get('n_contours', 0)} | a={a:.4f}\"\n",
    "            f\"{g['group_label']} | a={a:.4f}\"\n",
    "        )\n",
    "\n",
    "        axes[0].plot(x_fit, y_fit, '--', linewidth=2, color=color)\n",
    "        axes[0].plot(x_orig, y_orig, '-o', color=color, label=label)\n",
    "\n",
    "    axes[0].set_title(f'exponential fit, step={step}', fontsize=15)\n",
    "    axes[0].set_xlim(1, 2)\n",
    "    axes[0].set_ylim(1e-8, 1)\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].set_ylabel('p(x)', fontsize=15)\n",
    "    axes[0].legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    # Diagnostics: parameter a per group\n",
    "    axes[1].plot(range(len(groups)), a_list, '-o', label='a')\n",
    "    axes[1].set_title(f'exponential parameter a, step={step}', fontsize=15)\n",
    "    axes[1].set_xticks(range(len(groups)))\n",
    "    axes[1].set_xticklabels(group_labels, rotation=45, fontsize=12, ha='right')\n",
    "    axes[1].legend(loc='upper right', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wc-cv",
   "language": "python",
   "name": "wc-cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
